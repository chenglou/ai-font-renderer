"""
This file implements utilities for working with font datasets.

The module provides functions for:
- Loading BMP images generated by generate_font.ts
- Creating PyTorch datasets from disk
- Reading and writing BMP files
- Extracting sheet dimensions
"""

import os
import glob
import numpy as np
import torch
import torch.utils.data as data
from PIL import Image

def read_bmp_file(file_path):
    """Read a BMP file and convert to normalized grayscale array (0=black, 1=white)"""
    with Image.open(file_path) as img:
        # Convert to grayscale (if not already)
        if img.mode != 'L':
            img = img.convert('L')

        # Get dimensions and convert to numpy array
        width, height = img.size
        img_array = np.array(img)

        # Normalize to 0-1 (0=black, 1=white)
        img_array = img_array / 255.0

        return img_array.astype(np.float32)

def binary_array_to_image(binary_array, output_path=None):
    """
    Convert a grayscale array (where 0=black, 1=white) to a PIL Image.

    Args:
        binary_array (numpy.ndarray): A 2D array with values 0.0-1.0 where 0.0 represents black and 1.0 represents white
        output_path (str, optional): If provided, save the image to this path

    Returns:
        PIL.Image: The converted image object
    """
    # Convert grayscale array to image format (255 for white, 0 for black)
    # Scale from 0.0-1.0 to 0-255
    img = (binary_array * 255).astype(np.uint8)

    # Convert to PIL Image
    pil_img = Image.fromarray(img)

    # Save if path is provided
    if output_path:
        # Ensure directory exists
        os.makedirs(os.path.dirname(output_path) or '.', exist_ok=True)
        pil_img.save(output_path, "BMP")

    return pil_img

def get_sheet_dimensions(dataset_dir="train_input"):
    """Get sheet dimensions from the first BMP file in the dataset directory"""
    # Check for first sample (with new numbering scheme)
    sample_path = f"{dataset_dir}/1.bmp"

    # Fall back to old naming pattern if needed
    if not os.path.exists(sample_path):
        bmp_files = glob.glob(f"{dataset_dir}/input_*.bmp")
        if not bmp_files:
            raise ValueError(f"No BMP files found in {dataset_dir}")
        sample_path = bmp_files[0]

    # Read the BMP file
    img = read_bmp_file(sample_path)

    # Height and width from shape
    height, width = img.shape

    return height, width

def load_dataset(dataset_dir="train_input", validation_split=0.1):
    """Load the font dataset from disk using the data.txt file and numbered bmps format"""
    print(f"Loading font dataset from {dataset_dir}...")

    # Check if data.txt exists
    data_file = f"{dataset_dir}/data.txt"
    if not os.path.exists(data_file):
        raise ValueError(f"Data file not found: {data_file}")

    # Read all text labels
    with open(data_file, 'r') as f:
        text_labels = [line.strip() for line in f.readlines()]

    num_samples = len(text_labels)
    print(f"Found {num_samples} text labels in data.txt")

    # Sample first file to get dimensions
    sample_path = f"{dataset_dir}/1.bmp"
    if not os.path.exists(sample_path):
        raise ValueError(f"First sample image not found: {sample_path}")

    sample_img = read_bmp_file(sample_path)
    SHEET_HEIGHT, SHEET_WIDTH = sample_img.shape
    print(f"Sheet dimensions: {SHEET_WIDTH}x{SHEET_HEIGHT}")

    # Initialize arrays for dataset
    all_inputs = []
    all_targets = np.zeros((num_samples, SHEET_HEIGHT, SHEET_WIDTH), dtype=np.float32)

    # Process each sample
    for idx in range(num_samples):
        # File numbers are 1-indexed
        file_idx = idx + 1
        bmp_file = f"{dataset_dir}/{file_idx}.bmp"

        # Check if the file exists
        if not os.path.exists(bmp_file):
            raise ValueError(f"Image file not found: {bmp_file}")

        # Load image target
        all_targets[idx] = read_bmp_file(bmp_file)

        # Get text from pre-loaded labels
        text = text_labels[idx]

        # Convert to ASCII codes
        ascii_codes = [ord(c) for c in text]
        all_inputs.append(ascii_codes)

        if idx % (num_samples // 10) == 0:
            print(f"Loaded {idx}/{num_samples} samples...")

    # Find max length for padding
    max_len = max(len(s) for s in all_inputs)

    # Pad all sequences
    padded_inputs = np.zeros((num_samples, max_len), dtype=np.int64)
    for i, codes in enumerate(all_inputs):
        padded_inputs[i, :len(codes)] = codes

    # Convert to tensors
    inputs_tensor = torch.tensor(padded_inputs, dtype=torch.long)
    targets_tensor = torch.tensor(all_targets, dtype=torch.float32)

    # Create TensorDataset
    dataset = data.TensorDataset(inputs_tensor, targets_tensor)

    # Calculate max characters per sheet for model initialization
    max_chars_per_sheet = max_len

    print(f"Dataset loaded: {num_samples} samples with dimensions {SHEET_HEIGHT}x{SHEET_WIDTH}")
    print(f"Maximum characters per sheet: {max_chars_per_sheet}")

    return dataset, (SHEET_HEIGHT, SHEET_WIDTH), max_chars_per_sheet

# Get sheet dimensions and max characters per sheet - these can be imported directly
SHEET_HEIGHT, SHEET_WIDTH = get_sheet_dimensions()
MAX_CHARS_PER_SHEET = 100  # Default maximum, will be updated when loading the dataset

if __name__ == "__main__":
    """Run this file directly to test the functions"""
    print("Testing Montserrat helpers...")
    height, width = get_sheet_dimensions()
    print(f"Montserrat sheet dimensions: {width}x{height}")

    # Load a small dataset to test functionality
    dataset, (height, width), max_chars = load_montserrat_dataset()
    print(f"Test dataset loaded with {len(dataset)} samples")
    print(f"Maximum characters per sheet: {max_chars}")
